{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13042295,"sourceType":"datasetVersion","datasetId":8258645}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:34:28.140784Z","iopub.execute_input":"2026-02-17T15:34:28.141198Z","iopub.status.idle":"2026-02-17T15:34:36.236889Z","shell.execute_reply.started":"2026-02-17T15:34:28.141149Z","shell.execute_reply":"2026-02-17T15:34:36.234472Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\nDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.7.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import cKDTree","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:34:36.241296Z","iopub.execute_input":"2026-02-17T15:34:36.242098Z","iopub.status.idle":"2026-02-17T15:34:51.960351Z","shell.execute_reply.started":"2026-02-17T15:34:36.242042Z","shell.execute_reply":"2026-02-17T15:34:51.959050Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/datasets/firecastrl/us-wildfire-dataset/Wildfire_Dataset.csv', parse_dates=['datetime'])\n\nfeature_cols = ['pr', 'rmax', 'rmin', 'sph', 'srad', 'tmmn', 'tmmx',\n                'vs', 'bi', 'fm100', 'fm1000', 'erc', 'etr', 'pet', 'vpd']\ntarget_col = 'Wildfire'\n\n# Encode target once, vectorized\ndf['label'] = (df['Wildfire'].str.strip() == 'Yes').astype(np.float32)\n\n# Verify immediately\nprint(\"Label counts:\", df['label'].value_counts().to_dict())\nprint(\"Wildfire %:\", df['label'].mean() * 100, \"%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:43:22.600710Z","iopub.execute_input":"2026-02-17T15:43:22.601465Z","iopub.status.idle":"2026-02-17T15:43:47.808690Z","shell.execute_reply.started":"2026-02-17T15:43:22.601422Z","shell.execute_reply":"2026-02-17T15:43:47.807532Z"}},"outputs":[{"name":"stdout","text":"Label counts: {0.0: 9007860, 1.0: 502065}\nWildfire %: 5.2793794 %\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Get unique spatial locations and assign stable node IDs\ncoords = df[['latitude', 'longitude']].drop_duplicates().reset_index(drop=True)\ncoords['node_id'] = coords.index\nnum_nodes = len(coords)\n\n# Merge node_id back — O(n) hash join\ndf = df.merge(coords, on=['latitude', 'longitude'], how='left')\n\nprint(f\"Nodes: {num_nodes:,} | Timesteps: {df['datetime'].nunique():,} | Rows: {len(df):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:00.518693Z","iopub.execute_input":"2026-02-17T15:44:00.519817Z","iopub.status.idle":"2026-02-17T15:44:02.480111Z","shell.execute_reply.started":"2026-02-17T15:44:00.519767Z","shell.execute_reply":"2026-02-17T15:44:02.478969Z"}},"outputs":[{"name":"stdout","text":"Nodes: 37,098 | Timesteps: 4,122 | Rows: 9,509,925\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"scaler = StandardScaler()\ndf[feature_cols] = scaler.fit_transform(df[feature_cols]).astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:02.481708Z","iopub.execute_input":"2026-02-17T15:44:02.482242Z","iopub.status.idle":"2026-02-17T15:44:05.180569Z","shell.execute_reply.started":"2026-02-17T15:44:02.482205Z","shell.execute_reply":"2026-02-17T15:44:05.179514Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Build spatial graph: connect each node to its k nearest neighbors\nK = 6  # neighbors per node\ncoord_arr = coords[['latitude', 'longitude']].values\n\ntree = cKDTree(coord_arr)\ndistances, indices = tree.query(coord_arr, k=K + 1)  # +1 to exclude self\n\nsrc, dst = [], []\nfor i, neighbors in enumerate(indices):\n    for j in neighbors[1:]:  # skip self (index 0)\n        src.append(i)\n        dst.append(j)\n\nedge_index = torch.tensor([src, dst], dtype=torch.long)\nprint(f\"Edges: {edge_index.shape[1]:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:05.181866Z","iopub.execute_input":"2026-02-17T15:44:05.182320Z","iopub.status.idle":"2026-02-17T15:44:05.413868Z","shell.execute_reply.started":"2026-02-17T15:44:05.182275Z","shell.execute_reply":"2026-02-17T15:44:05.412842Z"}},"outputs":[{"name":"stdout","text":"Edges: 222,588\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Same flat index trick but process in date chunks to cap RAM\ndf['t_id'] = df['datetime'].rank(method='dense').astype(int) - 1\nnum_times = df['t_id'].nunique()\n\nCHUNK = 100  # process 100 dates at a time, tune up/down based on RAM\n\nsnapshots = []\n\nfor t_start in range(0, num_times, CHUNK):\n    t_end = min(t_start + CHUNK, num_times)\n    chunk_size = t_end - t_start\n\n    mask = (df['t_id'] >= t_start) & (df['t_id'] < t_end)\n    chunk = df.loc[mask]\n\n    local_t = (chunk['t_id'].values - t_start)\n    flat_idx = local_t * num_nodes + chunk['node_id'].values\n\n    X_chunk = np.zeros((chunk_size * num_nodes, len(feature_cols)), dtype=np.float32)\n    y_chunk = np.zeros(chunk_size * num_nodes, dtype=np.float32)\n\n    X_chunk[flat_idx] = chunk[feature_cols].values.astype(np.float32)\n    y_chunk[flat_idx] = chunk['label'].values\n\n    X_chunk = X_chunk.reshape(chunk_size, num_nodes, len(feature_cols))\n    y_chunk = y_chunk.reshape(chunk_size, num_nodes)\n\n    for i in range(chunk_size):\n        snapshots.append(Data(\n            x=torch.from_numpy(X_chunk[i]),\n            y=torch.from_numpy(y_chunk[i]),\n            edge_index=edge_index\n        ))\n\n    if (t_start // CHUNK) % 5 == 0:\n        print(f\"  {t_end}/{num_times} timesteps done...\")\n\nprint(f\"Done. {len(snapshots)} snapshots\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:05.415698Z","iopub.execute_input":"2026-02-17T15:44:05.416025Z","iopub.status.idle":"2026-02-17T15:44:17.210635Z","shell.execute_reply.started":"2026-02-17T15:44:05.415994Z","shell.execute_reply":"2026-02-17T15:44:17.209701Z"}},"outputs":[{"name":"stdout","text":"  100/4122 timesteps done...\n  600/4122 timesteps done...\n  1100/4122 timesteps done...\n  1600/4122 timesteps done...\n  2100/4122 timesteps done...\n  2600/4122 timesteps done...\n  3100/4122 timesteps done...\n  3600/4122 timesteps done...\n  4100/4122 timesteps done...\nDone. 4122 snapshots\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"n = len(snapshots)\ntrain_end = int(0.7 * n)\nval_end   = int(0.85 * n)\n\ntrain_snapshots = snapshots[:train_end]\nval_snapshots   = snapshots[train_end:val_end]\ntest_snapshots  = snapshots[val_end:]\n\nprint(f\"Train: {len(train_snapshots)} | Val: {len(val_snapshots)} | Test: {len(test_snapshots)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:17.211981Z","iopub.execute_input":"2026-02-17T15:44:17.212483Z","iopub.status.idle":"2026-02-17T15:44:17.782684Z","shell.execute_reply.started":"2026-02-17T15:44:17.212412Z","shell.execute_reply":"2026-02-17T15:44:17.781855Z"}},"outputs":[{"name":"stdout","text":"Train: 2885 | Val: 618 | Test: 619\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"snap = snapshots[0]\nprint(\"x shape     :\", snap.x.shape)       # [num_nodes, 15]\nprint(\"y shape     :\", snap.y.shape)       # [num_nodes]\nprint(\"edge_index  :\", snap.edge_index.shape)  # [2, num_edges]\nprint(\"Wildfire %  :\", snap.y.mean().item() * 100, \"%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:17.784548Z","iopub.execute_input":"2026-02-17T15:44:17.784886Z","iopub.status.idle":"2026-02-17T15:44:17.807086Z","shell.execute_reply.started":"2026-02-17T15:44:17.784854Z","shell.execute_reply":"2026-02-17T15:44:17.806135Z"}},"outputs":[{"name":"stdout","text":"x shape     : torch.Size([37098, 15])\ny shape     : torch.Size([37098])\nedge_index  : torch.Size([2, 222588])\nWildfire %  : 0.0 %\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Check what values actually exist in the Wildfire column\nprint(\"Raw value counts:\")\nprint(df['Wildfire'].value_counts())\n\nprint(\"\\nUnique values:\", df['Wildfire'].unique()[:20])\n\nprint(\"\\nDtype:\", df['Wildfire'].dtype)\n\n# Check what the label column looks like after encoding\nprint(\"\\nLabel distribution:\")\nprint(df['label'].value_counts())\n\nprint(\"\\nSample rows:\")\nprint(df[['Wildfire', 'label']].drop_duplicates().head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:44:17.808245Z","iopub.execute_input":"2026-02-17T15:44:17.808659Z","iopub.status.idle":"2026-02-17T15:44:20.022280Z","shell.execute_reply.started":"2026-02-17T15:44:17.808580Z","shell.execute_reply":"2026-02-17T15:44:20.021481Z"}},"outputs":[{"name":"stdout","text":"Raw value counts:\nWildfire\nNo     9007860\nYes     502065\nName: count, dtype: int64\n\nUnique values: ['No' 'Yes']\n\nDtype: object\n\nLabel distribution:\nlabel\n0.0    9007860\n1.0     502065\nName: count, dtype: int64\n\nSample rows:\n    Wildfire  label\n0         No    0.0\n285      Yes    1.0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Run this BEFORE re-running Cell 6\nprint(\"Label sum in df:\", df['label'].sum())          # should be 502065\nprint(\"Label in flat array test:\")\n\nt0_mask = df['t_id'] == 0\nprint(\"  t=0 positives:\", df.loc[t0_mask, 'label'].sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:45:58.142190Z","iopub.execute_input":"2026-02-17T15:45:58.143435Z","iopub.status.idle":"2026-02-17T15:45:58.170987Z","shell.execute_reply.started":"2026-02-17T15:45:58.143392Z","shell.execute_reply":"2026-02-17T15:45:58.169669Z"}},"outputs":[{"name":"stdout","text":"Label sum in df: 502065.0\nLabel in flat array test:\n  t=0 positives: 0.0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"positives_per_t = df.groupby('t_id')['label'].sum()\n\nprint(\"Timesteps with ANY wildfire:\", (positives_per_t > 0).sum(), \"out of\", len(positives_per_t))\nprint(\"\\nFirst 10 timesteps with fires:\")\nprint(positives_per_t[positives_per_t > 0].head(10))\n\nprint(\"\\nDate of first wildfire timestep:\")\nfirst_fire_t = positives_per_t[positives_per_t > 0].index[0]\nprint(df[df['t_id'] == first_fire_t]['datetime'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:46:24.000758Z","iopub.execute_input":"2026-02-17T15:46:24.002098Z","iopub.status.idle":"2026-02-17T15:46:24.194955Z","shell.execute_reply.started":"2026-02-17T15:46:24.002051Z","shell.execute_reply":"2026-02-17T15:46:24.193640Z"}},"outputs":[{"name":"stdout","text":"Timesteps with ANY wildfire: 3994 out of 4122\n\nFirst 10 timesteps with fires:\nt_id\n69    1.0\n70    1.0\n71    1.0\n72    1.0\n73    1.0\n74    1.0\n75    1.0\n76    1.0\n77    1.0\n78    1.0\nName: label, dtype: float32\n\nDate of first wildfire timestep:\n2014-03-10 00:00:00\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Check what date snapshot[0] corresponds to\nprint(\"Snapshot[0] date:\", df[df['t_id'] == 0]['datetime'].iloc[0])\nprint(\"Snapshot[69] date:\", df[df['t_id'] == 69]['datetime'].iloc[0])\n\n# Overall health check across ALL snapshots\ntotal_positives = sum(s.y.sum().item() for s in snapshots)\nsnapshots_with_fire = sum(1 for s in snapshots if s.y.sum() > 0)\n\nprint(f\"\\n=== Pipeline Health Check ===\")\nprint(f\"Total snapshots       : {len(snapshots)}\")\nprint(f\"Snapshots with fires  : {snapshots_with_fire} / {len(snapshots)}\")\nprint(f\"Total positive labels : {total_positives:,.0f}  (expected ~502,065)\")\nprint(f\"Overall Wildfire %    : {total_positives / (len(snapshots) * num_nodes) * 100:.3f}%\")\n\n# Spot check a snapshot that has fires\nfire_snap = snapshots[69]\nprint(f\"\\nSnapshot[69] Wildfire %: {fire_snap.y.mean().item()*100:.3f}%\")\nprint(f\"Snapshot[69] x shape  : {fire_snap.x.shape}\")\nprint(f\"Snapshot[69] y shape  : {fire_snap.y.shape}\")\nprint(f\"Snapshot[69] edges    : {fire_snap.edge_index.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:46:48.914321Z","iopub.execute_input":"2026-02-17T15:46:48.914712Z","iopub.status.idle":"2026-02-17T15:46:49.244361Z","shell.execute_reply.started":"2026-02-17T15:46:48.914674Z","shell.execute_reply":"2026-02-17T15:46:49.243288Z"}},"outputs":[{"name":"stdout","text":"Snapshot[0] date: 2013-12-31 00:00:00\nSnapshot[69] date: 2014-03-10 00:00:00\n\n=== Pipeline Health Check ===\nTotal snapshots       : 4122\nSnapshots with fires  : 3994 / 4122\nTotal positive labels : 498,661  (expected ~502,065)\nOverall Wildfire %    : 0.326%\n\nSnapshot[69] Wildfire %: 0.003%\nSnapshot[69] x shape  : torch.Size([37098, 15])\nSnapshot[69] y shape  : torch.Size([37098])\nSnapshot[69] edges    : torch.Size([2, 222588])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass TGNN(nn.Module):\n    def __init__(self, in_channels, hidden_channels, gru_hidden, num_layers=2):\n        super().__init__()\n        \n        # Spatial: stack of GCN layers\n        self.gcn_layers = nn.ModuleList()\n        self.gcn_layers.append(GCNConv(in_channels, hidden_channels))\n        for _ in range(num_layers - 1):\n            self.gcn_layers.append(GCNConv(hidden_channels, hidden_channels))\n        \n        # Temporal: GRU over spatial embeddings\n        self.gru = nn.GRU(hidden_channels, gru_hidden, batch_first=True, num_layers=2)\n        \n        # Classifier head\n        self.classifier = nn.Sequential(\n            nn.Linear(gru_hidden, gru_hidden // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(gru_hidden // 2, 1)\n        )\n    \n    def forward(self, snapshot_seq, edge_index):\n        \"\"\"\n        snapshot_seq: [T, N, F]  (sequence of T snapshots)\n        edge_index:   [2, E]\n        returns:      [N] logits for final timestep\n        \"\"\"\n        spatial_embeds = []\n        \n        for t in range(snapshot_seq.size(0)):\n            x = snapshot_seq[t]                          # [N, F]\n            for gcn in self.gcn_layers:\n                x = F.relu(gcn(x, edge_index))           # [N, H]\n            spatial_embeds.append(x)\n        \n        # Stack: [N, T, H]\n        x = torch.stack(spatial_embeds, dim=1)\n        \n        # GRU: [N, T, H] -> [N, T, gru_hidden]\n        x, _ = self.gru(x)\n        \n        # Take last timestep: [N, gru_hidden]\n        x = x[:, -1, :]\n        \n        # Classify: [N]\n        return self.classifier(x).squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:48:06.845780Z","iopub.execute_input":"2026-02-17T15:48:06.846545Z","iopub.status.idle":"2026-02-17T15:48:06.856095Z","shell.execute_reply.started":"2026-02-17T15:48:06.846507Z","shell.execute_reply":"2026-02-17T15:48:06.855131Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\nmodel = TGNN(\n    in_channels=15,\n    hidden_channels=64,\n    gru_hidden=64,\n    num_layers=2\n).to(device)\n\n# Class imbalance weight (~306:1 at node level, use softer weight)\npos_weight = torch.tensor([9007860 / 502065]).to(device)  # ~17.9\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n\nprint(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:48:15.388403Z","iopub.execute_input":"2026-02-17T15:48:15.389554Z","iopub.status.idle":"2026-02-17T15:48:15.464099Z","shell.execute_reply.started":"2026-02-17T15:48:15.389488Z","shell.execute_reply":"2026-02-17T15:48:15.462725Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nModel params: 57,217\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"SEQ_LEN = 2        # use 7 days of history to predict next day\nEPOCHS  = 2\nBATCH_T = 2        # process 4 windows at a time (tune for your GPU RAM)\n\nedge_index_dev = edge_index.to(device)\n\ndef make_windows(split_snapshots, seq_len):\n    \"\"\"Yield (input_seq [T,N,F], target [N]) windows\"\"\"\n    for i in range(len(split_snapshots) - seq_len):\n        xs = torch.stack([s.x for s in split_snapshots[i:i+seq_len]])   # [T,N,F]\n        y  = split_snapshots[i + seq_len].y                              # [N]\n        yield xs, y\n\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    # --- Train ---\n    model.train()\n    train_loss, train_steps = 0.0, 0\n    \n    batch_xs, batch_ys = [], []\n    for xs, y in make_windows(train_snapshots, SEQ_LEN):\n        batch_xs.append(xs)\n        batch_ys.append(y)\n        \n        if len(batch_xs) == BATCH_T:\n            optimizer.zero_grad()\n            loss = torch.tensor(0.0, device=device)\n            \n            for bx, by in zip(batch_xs, batch_ys):\n                bx = bx.to(device)\n                by = by.to(device)\n                logits = model(bx, edge_index_dev)\n                loss += criterion(logits, by)\n            \n            (loss / BATCH_T).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_steps += BATCH_T\n            batch_xs, batch_ys = [], []\n    \n    # --- Validate ---\n    model.eval()\n    val_loss, val_steps = 0.0, 0\n    \n    with torch.no_grad():\n        for xs, y in make_windows(val_snapshots, SEQ_LEN):\n            xs, y = xs.to(device), y.to(device)\n            logits = model(xs, edge_index_dev)\n            val_loss += criterion(logits, y).item()\n            val_steps += 1\n    \n    avg_train = train_loss / max(train_steps, 1)\n    avg_val   = val_loss   / max(val_steps, 1)\n    scheduler.step(avg_val)\n    \n    if avg_val < best_val_loss:\n        best_val_loss = avg_val\n        torch.save(model.state_dict(), 'best_tgnn.pt')\n        tag = \" ← best\"\n    else:\n        tag = \"\"\n    \n    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}{tag}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T16:05:57.024044Z","iopub.execute_input":"2026-02-17T16:05:57.024422Z","iopub.status.idle":"2026-02-17T16:06:07.969270Z","shell.execute_reply.started":"2026-02-17T16:05:57.024386Z","shell.execute_reply":"2026-02-17T16:06:07.968025Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/331442049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, classification_report\n\nmodel.load_state_dict(torch.load('best_tgnn.pt'))\nmodel.eval()\n\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for xs, y in make_windows(test_snapshots, SEQ_LEN):\n        xs = xs.to(device)\n        logits = model(xs, edge_index_dev)\n        probs  = torch.sigmoid(logits).cpu().numpy()\n        all_preds.append(probs)\n        all_labels.append(y.numpy())\n\nall_preds  = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\n# Tune threshold for imbalanced data (default 0.5 is usually too high)\nthreshold = 0.3\nbinary_preds = (all_preds >= threshold).astype(int)\n\nprint(classification_report(all_labels, binary_preds, target_names=['No Fire', 'Wildfire']))\nprint(f\"ROC-AUC : {roc_auc_score(all_labels, all_preds):.4f}\")\nprint(f\"F1      : {f1_score(all_labels, binary_preds):.4f}\")\nprint(f\"Recall  : {recall_score(all_labels, binary_preds):.4f}\")\nprint(f\"Precision:{precision_score(all_labels, binary_preds):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}