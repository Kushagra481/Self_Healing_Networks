{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Graph"
      ],
      "metadata": {
        "id": "fFa2Vsg0RM-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NX4VjAVzIfNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c1c73f-5de8-42d0-eb18-14948e4300c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating disaster network dataset...\n",
            "Generated 1283 events\n",
            "Current nodes: 10\n",
            "Current edges: 0\n",
            "\n",
            "Training TGN model...\n",
            "Epoch 1/20, Loss: 0.6955\n",
            "Epoch 2/20, Loss: 0.5652\n",
            "Epoch 3/20, Loss: 0.4541\n",
            "Epoch 4/20, Loss: 0.4172\n",
            "Epoch 5/20, Loss: 0.3263\n",
            "Epoch 6/20, Loss: 0.2790\n",
            "Epoch 7/20, Loss: 0.2059\n",
            "Epoch 8/20, Loss: 0.1854\n",
            "Epoch 9/20, Loss: 0.1356\n",
            "Epoch 10/20, Loss: 0.1335\n",
            "Epoch 11/20, Loss: 0.1183\n",
            "Epoch 12/20, Loss: 0.1174\n",
            "Epoch 13/20, Loss: 0.1333\n",
            "Epoch 14/20, Loss: 0.1105\n",
            "Epoch 15/20, Loss: 0.0994\n",
            "Epoch 16/20, Loss: 0.0967\n",
            "Epoch 17/20, Loss: 0.0779\n",
            "Epoch 18/20, Loss: 0.1211\n",
            "Epoch 19/20, Loss: 0.1219\n",
            "Epoch 20/20, Loss: 0.0958\n",
            "\n",
            "Predicting link failures...\n",
            "\n",
            "Found 0 at-risk links:\n",
            "\n",
            "Executing self-healing actions...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, node_id, features, timestamp):\n",
        "        self.id = node_id\n",
        "        self.features = np.array(features, dtype=np.float32)\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "class Edge:\n",
        "    def __init__(self, source_id, target_id, features, timestamp):\n",
        "        self.source_id = source_id\n",
        "        self.target_id = target_id\n",
        "        self.features = np.array(features, dtype=np.float32)\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "class Event:\n",
        "    def __init__(self, timestamp, event_type, obj_id, old_features, new_features):\n",
        "        self.timestamp = timestamp\n",
        "        self.event_type = event_type\n",
        "        self.obj_id = obj_id\n",
        "        self.old_features = old_features\n",
        "        self.new_features = new_features\n",
        "\n",
        "class TemporalGraph:\n",
        "    def __init__(self):\n",
        "        self.nodes = {}\n",
        "        self.edges = {}\n",
        "        self.event_stream = []\n",
        "        self.current_time = 0\n",
        "\n",
        "    def add_node(self, node_id, features, timestamp):\n",
        "        if node_id in self.nodes:\n",
        "            return False\n",
        "        node = Node(node_id, features, timestamp)\n",
        "        self.nodes[node_id] = node\n",
        "        event = Event(timestamp, 'NODE_JOIN', node_id, None, features)\n",
        "        self.event_stream.append(event)\n",
        "        return True\n",
        "\n",
        "    def add_edge(self, source_id, target_id, features, timestamp):\n",
        "        if source_id not in self.nodes or target_id not in self.nodes:\n",
        "            return False\n",
        "        edge_key = (source_id, target_id)\n",
        "        edge = Edge(source_id, target_id, features, timestamp)\n",
        "        self.edges[edge_key] = edge\n",
        "        self.edges[(target_id, source_id)] = edge\n",
        "        event = Event(timestamp, 'EDGE_CREATED', edge_key, None, features)\n",
        "        self.event_stream.append(event)\n",
        "        return True\n",
        "\n",
        "    def update_node_features(self, node_id, new_features, timestamp):\n",
        "        if node_id not in self.nodes:\n",
        "            return False\n",
        "        old_features = self.nodes[node_id].features.copy()\n",
        "        self.nodes[node_id].features = np.array(new_features, dtype=np.float32)\n",
        "        self.nodes[node_id].timestamp = timestamp\n",
        "        event = Event(timestamp, 'NODE_UPDATE', node_id, old_features, new_features)\n",
        "        self.event_stream.append(event)\n",
        "        return True\n",
        "\n",
        "    def update_edge_features(self, source_id, target_id, new_features, timestamp):\n",
        "        edge_key = (source_id, target_id)\n",
        "        if edge_key not in self.edges:\n",
        "            return False\n",
        "        old_features = self.edges[edge_key].features.copy()\n",
        "        self.edges[edge_key].features = np.array(new_features, dtype=np.float32)\n",
        "        self.edges[edge_key].timestamp = timestamp\n",
        "        self.edges[(target_id, source_id)].features = np.array(new_features, dtype=np.float32)\n",
        "        event = Event(timestamp, 'EDGE_UPDATE', edge_key, old_features, new_features)\n",
        "        self.event_stream.append(event)\n",
        "        return True\n",
        "\n",
        "    def remove_edge(self, source_id, target_id, timestamp):\n",
        "        edge_key = (source_id, target_id)\n",
        "        if edge_key not in self.edges:\n",
        "            return False\n",
        "        old_features = self.edges[edge_key].features.copy()\n",
        "        del self.edges[edge_key]\n",
        "        del self.edges[(target_id, source_id)]\n",
        "        event = Event(timestamp, 'EDGE_DELETED', edge_key, old_features, None)\n",
        "        self.event_stream.append(event)\n",
        "        return True\n",
        "\n",
        "    def get_neighbors(self, node_id):\n",
        "        neighbors = []\n",
        "        for (src, tgt) in self.edges.keys():\n",
        "            if src == node_id:\n",
        "                neighbors.append(tgt)\n",
        "        return list(set(neighbors))\n",
        "\n",
        "class TimeEncoder(nn.Module):\n",
        "    def __init__(self, dimension):\n",
        "        super(TimeEncoder, self).__init__()\n",
        "        self.dimension = dimension\n",
        "        self.w = nn.Linear(1, dimension)\n",
        "\n",
        "    def forward(self, t):\n",
        "        t = t.unsqueeze(dim=1)\n",
        "        output = torch.cos(self.w(t))\n",
        "        return output\n",
        "\n",
        "class MessageFunction(nn.Module):\n",
        "    def __init__(self, node_feat_dim, edge_feat_dim, memory_dim, time_dim, message_dim):\n",
        "        super(MessageFunction, self).__init__()\n",
        "        input_dim = 2 * node_feat_dim + edge_feat_dim + 2 * memory_dim + time_dim\n",
        "        self.fc1 = nn.Linear(input_dim, message_dim)\n",
        "        self.fc2 = nn.Linear(message_dim, message_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, source_feat, target_feat, edge_feat, source_mem, target_mem, time_enc):\n",
        "        x = torch.cat([source_feat, target_feat, edge_feat, source_mem, target_mem, time_enc], dim=-1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MemoryUpdater(nn.Module):\n",
        "    def __init__(self, memory_dim, message_dim):\n",
        "        super(MemoryUpdater, self).__init__()\n",
        "        self.memory_dim = memory_dim\n",
        "        self.gru = nn.GRUCell(message_dim, memory_dim)\n",
        "\n",
        "    def forward(self, memory, message):\n",
        "        new_memory = self.gru(message, memory)\n",
        "        return new_memory\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self, memory_dim, edge_feat_dim, time_dim):\n",
        "        super(Predictor, self).__init__()\n",
        "        input_dim = 2 * memory_dim + edge_feat_dim + time_dim\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, source_mem, target_mem, edge_feat, time_enc):\n",
        "        x = torch.cat([source_mem, target_mem, edge_feat, time_enc], dim=-1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "class TGN(nn.Module):\n",
        "    def __init__(self, num_nodes, node_feat_dim, edge_feat_dim, memory_dim, time_dim, message_dim):\n",
        "        super(TGN, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.memory_dim = memory_dim\n",
        "        self.memory = torch.zeros(num_nodes, memory_dim)\n",
        "\n",
        "        self.time_encoder = TimeEncoder(time_dim)\n",
        "        self.message_function = MessageFunction(node_feat_dim, edge_feat_dim, memory_dim, time_dim, message_dim)\n",
        "        self.memory_updater = MemoryUpdater(memory_dim, message_dim)\n",
        "        self.predictor = Predictor(memory_dim, edge_feat_dim, time_dim)\n",
        "\n",
        "    def reset_memory(self):\n",
        "        self.memory = torch.zeros(self.num_nodes, self.memory_dim)\n",
        "\n",
        "    def forward(self, source_ids, target_ids, source_feats, target_feats, edge_feats, timestamps):\n",
        "        time_enc = self.time_encoder(timestamps)\n",
        "\n",
        "        source_mems = self.memory[source_ids]\n",
        "        target_mems = self.memory[target_ids]\n",
        "\n",
        "        messages_source = self.message_function(source_feats, target_feats, edge_feats,\n",
        "                                                source_mems, target_mems, time_enc)\n",
        "        messages_target = self.message_function(target_feats, source_feats, edge_feats,\n",
        "                                                target_mems, source_mems, time_enc)\n",
        "\n",
        "        new_source_mems = self.memory_updater(source_mems, messages_source)\n",
        "        new_target_mems = self.memory_updater(target_mems, messages_target)\n",
        "\n",
        "        self.memory[source_ids] = new_source_mems.detach()\n",
        "        self.memory[target_ids] = new_target_mems.detach()\n",
        "\n",
        "        predictions = self.predictor(new_source_mems, new_target_mems, edge_feats, time_enc)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "def generate_disaster_dataset(duration, num_nodes):\n",
        "    graph = TemporalGraph()\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        features = [\n",
        "            random.uniform(50, 100),\n",
        "            random.choice([0, 1, 2, 3]),\n",
        "            random.uniform(0, 500),\n",
        "            random.uniform(0, 500),\n",
        "            0\n",
        "        ]\n",
        "        graph.add_node(i, features, timestamp=0)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(i+1, num_nodes):\n",
        "            if random.random() < 0.3:\n",
        "                features = [\n",
        "                    random.uniform(-80, -50),\n",
        "                    random.uniform(0, 0.1),\n",
        "                    random.uniform(10, 50)\n",
        "                ]\n",
        "                graph.add_edge(i, j, features, timestamp=0)\n",
        "\n",
        "    for t in range(1, duration):\n",
        "        for node_id in list(graph.nodes.keys()):\n",
        "            node = graph.nodes[node_id]\n",
        "            new_battery = max(0, node.features[0] - random.uniform(0, 2))\n",
        "            new_features = node.features.copy()\n",
        "            new_features[0] = new_battery\n",
        "            if new_battery < 20:\n",
        "                new_features[4] = 1\n",
        "            graph.update_node_features(node_id, new_features, timestamp=t)\n",
        "\n",
        "        for edge_key in list(graph.edges.keys()):\n",
        "            if edge_key[0] < edge_key[1]:\n",
        "                edge = graph.edges[edge_key]\n",
        "                new_signal = edge.features[0] - random.uniform(0, 3)\n",
        "                new_features = edge.features.copy()\n",
        "                new_features[0] = new_signal\n",
        "                graph.update_edge_features(edge_key[0], edge_key[1], new_features, timestamp=t)\n",
        "\n",
        "                if new_signal < -90:\n",
        "                    graph.remove_edge(edge_key[0], edge_key[1], timestamp=t)\n",
        "\n",
        "        if random.random() < 0.05:\n",
        "            active_nodes = [nid for nid, n in graph.nodes.items() if n.features[4] == 0]\n",
        "            if active_nodes:\n",
        "                failed_node = random.choice(active_nodes)\n",
        "                new_features = graph.nodes[failed_node].features.copy()\n",
        "                new_features[4] = 2\n",
        "                graph.update_node_features(failed_node, new_features, timestamp=t)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def prepare_training_data(graph, prediction_horizon=10):\n",
        "    data = []\n",
        "\n",
        "    for i, event in enumerate(graph.event_stream):\n",
        "        if event.event_type in ['EDGE_UPDATE', 'EDGE_CREATED']:\n",
        "            source_id, target_id = event.obj_id\n",
        "\n",
        "            if source_id not in graph.nodes or target_id not in graph.nodes:\n",
        "                continue\n",
        "\n",
        "            source_node = graph.nodes[source_id]\n",
        "            target_node = graph.nodes[target_id]\n",
        "\n",
        "            future_time = event.timestamp + prediction_horizon\n",
        "            label = 0\n",
        "\n",
        "            for future_event in graph.event_stream[i:]:\n",
        "                if future_event.timestamp > future_time:\n",
        "                    break\n",
        "                if future_event.event_type == 'EDGE_DELETED' and future_event.obj_id == event.obj_id:\n",
        "                    label = 1\n",
        "                    break\n",
        "                if future_event.event_type == 'EDGE_UPDATE' and future_event.obj_id == event.obj_id:\n",
        "                    if future_event.new_features[0] < -90:\n",
        "                        label = 1\n",
        "                        break\n",
        "\n",
        "            data.append({\n",
        "                'source_id': source_id,\n",
        "                'target_id': target_id,\n",
        "                'source_features': source_node.features,\n",
        "                'target_features': target_node.features,\n",
        "                'edge_features': event.new_features,\n",
        "                'timestamp': event.timestamp,\n",
        "                'label': label\n",
        "            })\n",
        "\n",
        "    return data\n",
        "\n",
        "def train_tgn(graph, num_epochs=50, learning_rate=0.001, batch_size=32):\n",
        "    num_nodes = len(graph.nodes)\n",
        "    node_feat_dim = 5\n",
        "    edge_feat_dim = 3\n",
        "    memory_dim = 64\n",
        "    time_dim = 32\n",
        "    message_dim = 32\n",
        "\n",
        "    model = TGN(num_nodes, node_feat_dim, edge_feat_dim, memory_dim, time_dim, message_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    training_data = prepare_training_data(graph, prediction_horizon=10)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.reset_memory()\n",
        "        random.shuffle(training_data)\n",
        "\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i in range(0, len(training_data), batch_size):\n",
        "            batch = training_data[i:i+batch_size]\n",
        "\n",
        "            source_ids = torch.tensor([d['source_id'] for d in batch], dtype=torch.long)\n",
        "            target_ids = torch.tensor([d['target_id'] for d in batch], dtype=torch.long)\n",
        "            source_feats = torch.tensor(np.array([d['source_features'] for d in batch]), dtype=torch.float32)\n",
        "            target_feats = torch.tensor(np.array([d['target_features'] for d in batch]), dtype=torch.float32)\n",
        "            edge_feats = torch.tensor(np.array([d['edge_features'] for d in batch]), dtype=torch.float32)\n",
        "            timestamps = torch.tensor([d['timestamp'] for d in batch], dtype=torch.float32)\n",
        "            labels = torch.tensor([[d['label']] for d in batch], dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(source_ids, target_ids, source_feats, target_feats, edge_feats, timestamps)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_link_failures(model, graph, current_time, threshold=0.7):\n",
        "    at_risk_links = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for edge_key in graph.edges.keys():\n",
        "            if edge_key[0] < edge_key[1]:\n",
        "                source_id, target_id = edge_key\n",
        "                edge = graph.edges[edge_key]\n",
        "                source_node = graph.nodes[source_id]\n",
        "                target_node = graph.nodes[target_id]\n",
        "\n",
        "                source_ids = torch.tensor([source_id], dtype=torch.long)\n",
        "                target_ids = torch.tensor([target_id], dtype=torch.long)\n",
        "                source_feats = torch.tensor([source_node.features], dtype=torch.float32)\n",
        "                target_feats = torch.tensor([target_node.features], dtype=torch.float32)\n",
        "                edge_feats = torch.tensor([edge.features], dtype=torch.float32)\n",
        "                timestamps = torch.tensor([current_time], dtype=torch.float32)\n",
        "\n",
        "                prediction = model(source_ids, target_ids, source_feats, target_feats, edge_feats, timestamps)\n",
        "\n",
        "                if prediction.item() > threshold:\n",
        "                    at_risk_links.append({\n",
        "                        'edge': edge_key,\n",
        "                        'probability': prediction.item(),\n",
        "                        'source_battery': source_node.features[0],\n",
        "                        'target_battery': target_node.features[0],\n",
        "                        'signal_strength': edge.features[0]\n",
        "                    })\n",
        "\n",
        "    at_risk_links.sort(key=lambda x: x['probability'], reverse=True)\n",
        "    return at_risk_links\n",
        "\n",
        "def find_alternative_path(graph, source, target, avoid_edge):\n",
        "    distances = {node_id: float('inf') for node_id in graph.nodes.keys()}\n",
        "    distances[source] = 0\n",
        "    previous = {node_id: None for node_id in graph.nodes.keys()}\n",
        "    unvisited = set(graph.nodes.keys())\n",
        "\n",
        "    while unvisited:\n",
        "        current = min(unvisited, key=lambda node: distances[node])\n",
        "\n",
        "        if distances[current] == float('inf'):\n",
        "            break\n",
        "\n",
        "        if current == target:\n",
        "            break\n",
        "\n",
        "        unvisited.remove(current)\n",
        "\n",
        "        for neighbor in graph.get_neighbors(current):\n",
        "            if neighbor not in graph.nodes:\n",
        "                continue\n",
        "\n",
        "            edge_key = (current, neighbor)\n",
        "            if edge_key == avoid_edge or (neighbor, current) == avoid_edge:\n",
        "                continue\n",
        "\n",
        "            if edge_key not in graph.edges:\n",
        "                continue\n",
        "\n",
        "            edge = graph.edges[edge_key]\n",
        "            weight = 1.0 / (abs(edge.features[0]) + 1)\n",
        "\n",
        "            alt_distance = distances[current] + weight\n",
        "\n",
        "            if alt_distance < distances[neighbor]:\n",
        "                distances[neighbor] = alt_distance\n",
        "                previous[neighbor] = current\n",
        "\n",
        "    if distances[target] == float('inf'):\n",
        "        return None\n",
        "\n",
        "    path = []\n",
        "    current = target\n",
        "    while current is not None:\n",
        "        path.append(current)\n",
        "        current = previous[current]\n",
        "    path.reverse()\n",
        "\n",
        "    return path\n",
        "\n",
        "def execute_self_healing(graph, at_risk_links, current_time):\n",
        "    actions = []\n",
        "\n",
        "    for risky_link in at_risk_links:\n",
        "        source, target = risky_link['edge']\n",
        "\n",
        "        alt_path = find_alternative_path(graph, source, target, risky_link['edge'])\n",
        "\n",
        "        if alt_path and len(alt_path) > 2:\n",
        "            action = {\n",
        "                'type': 'REROUTE',\n",
        "                'from_edge': (source, target),\n",
        "                'to_path': alt_path,\n",
        "                'timestamp': current_time,\n",
        "                'risk': risky_link['probability']\n",
        "            }\n",
        "            actions.append(action)\n",
        "        else:\n",
        "            action = {\n",
        "                'type': 'INCREASE_POWER',\n",
        "                'edge': (source, target),\n",
        "                'timestamp': current_time,\n",
        "                'risk': risky_link['probability']\n",
        "            }\n",
        "            actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Generating disaster network dataset...\")\n",
        "    graph = generate_disaster_dataset(duration=100, num_nodes=10)\n",
        "    print(f\"Generated {len(graph.event_stream)} events\")\n",
        "    print(f\"Current nodes: {len(graph.nodes)}\")\n",
        "    print(f\"Current edges: {len(graph.edges)//2}\")\n",
        "\n",
        "    print(\"\\nTraining TGN model...\")\n",
        "    model = train_tgn(graph, num_epochs=20, learning_rate=0.001, batch_size=16)\n",
        "\n",
        "    print(\"\\nPredicting link failures...\")\n",
        "    current_time = 95\n",
        "    at_risk = predict_link_failures(model, graph, current_time, threshold=0.6)\n",
        "\n",
        "    print(f\"\\nFound {len(at_risk)} at-risk links:\")\n",
        "    for link in at_risk[:5]:\n",
        "        print(f\"Edge {link['edge']}: {link['probability']:.3f} failure probability\")\n",
        "        print(f\"  Signal: {link['signal_strength']:.1f} dBm\")\n",
        "        print(f\"  Battery: {link['source_battery']:.1f}% / {link['target_battery']:.1f}%\")\n",
        "\n",
        "    print(\"\\nExecuting self-healing actions...\")\n",
        "    actions = execute_self_healing(graph, at_risk[:3], current_time)\n",
        "\n",
        "    for action in actions:\n",
        "        print(f\"\\nAction: {action['type']}\")\n",
        "        if action['type'] == 'REROUTE':\n",
        "            print(f\"  From: {action['from_edge']}\")\n",
        "            print(f\"  Path: {' -> '.join(map(str, action['to_path']))}\")\n",
        "        else:\n",
        "            print(f\"  Edge: {action['edge']}\")\n",
        "        print(f\"  Risk: {action['risk']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3fvTwjKSsfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}